\documentclass[dissertation.tex]{subfiles}
\begin{document}

\paragraph{Abstract}
Neural Networks (NN) are an extremely successful tool, they are widely adopted
commercially and closely studied academically, however even given the attention
they have there is no comprehensive understanding of how these models generalize
data and provide such impressive performance -- in fact very little is known
about how NN learn or about their inner workings. 

Recently Prof.\ Tishby
\cite{TISHBY} produced a paper claiming to understand the basic principles of
how NN work. He decided to examine NNs through the information domain. Tishby
made the claim that the incredible performance of NNs is due to their ability to
compress information. Compressing data means the network is only able to keep
relevant input features and it must discard the irrelevant bits of information,
leading to ability to generalize. 

Tishby made interesting claims and provided experimental evidence to support his
claims. However, he did not provide a formal proof leaving his results up for
debate. A paper release by Saxe \cite{SAXE} has contested the claims made by
Tishby arguing that compression cannot happen in Neural Networks and the results
are a consequence of the Neural Network activation function Tishby used.
However, Saxe's paper suffers from the same problem as Tishby's as it does not
provide a formal proof only experimental evidence -- as such it does not settle
the rebuttal.

\paragraph{Our Contribution}
The project was originally meant to reproduce the experiments presented in
Tishby's paper and show that they are robust -- not just an epiphenomenon of
Tishby's specific hyperparameters.  In this project I have successfully
implemented tools that are able to reproduce Tishby's and Saxe's experiments, as
well as leaving them open to extensions for future experiments.  Thus I was able
reproduced Tishby's experiments -- and manged to achieve the same results, and
showed that they are robust to changes in hyperparameters. 

However, Tishby's method for estimating Mutual Information (MI) are quite
primitive -- so the project evolved to be an exploration of compression in
Neural Networks. I Explored the role of Noise in Neural Networks and Studied
different Methods of MI Estimation. My Supervisor and Me devised a method to
potentially improve performance of existing MI estimation techniques -- called
Batching.

Overall, the project was a success.  However, the results we got are not
conclusive and opened up more questions and doubts.  More work needs to be done
on this topic.  Specifically, on mutual information estimation -- the tools
currently available are primitive and not fit to answer questions, that Tishby
and Saxe claim to have the answers to.

\end{document}
