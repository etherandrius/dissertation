Deep Neural Networks (DNNs) are an extremely successful tool, they are widely
adopted commercially and closely studied academically, however even given the
attention they have gathered there is no comprehensive understanding of how
these models generalize data and provide such impressive performance - in fact
very little is know about how DNNs learn or about their inner workings. Recently
Prof.\ Tishby produced a paper claiming to understand the basic principle of how
DNNs work. He suggested that there are two phases that the network goes trough
while being trained - the fitting phase and the compression phase. During the
fitting phase the network memorizes the training data and makes predictions
based on what it has seen before, during the compression phase the network
generalizes, it forgets the unnecessary information from the training data.
Tishby suggested that the incredible performance that DNNs are able to achieve
is due to this compression phase, and that this process of compression is a
result of randomness inherent in Stochastic gradient descent. Tishby showed this
by looking at DNNs trough the information domain, most notably he used what is
now called the information plane. The information plane summarizes how the
information is flowing trough the DNN, for every neuron layer the plane shows
mutual information it has with the input data and the label data. In his
experiments Tishby has concluded that every layer loses unnecessary information
from the input data and tries to keep information of the label.
%TODO(ag939) possibly explain what an information plane is here.
Tishby made some interesting and significant claims about how DNNs work, however
he did no provide a formal proof, his conclusion are based only on experimental
evidence. 

In our work we look at Tishby`s claim that DNNs compress data and throw away
unnecessary information about the input. We reimplement his experiments as a
form of independent verification, showing that the results Tishby got are robust
and are stable to parameter changes. We also take a look at a paper produced by
Saxe that provides an opposing view to that of Tishby`s. Saxe showed that
compression that Tishby showed is only a result of Tishby`s choice of
activation function for the neural network. He showed that compression only
happens when Tanh activation function is used and does not happen when ReLu is
used. 

Lastly, We think Tishby`s experiments don't fully align with some of the ideas
he presented, specifically his idea that weights should be treated as if they
are random. We devised an experiment that tries to capture this idea more
explicitly that Tishby`s experiments.
