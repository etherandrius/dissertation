Deep Neural Networks (DNNs) although extremely successful and widely adopted are
still a black box machine as there is no comprehensive understanding on how they
learn. Recently Prof.\ Tishby produced a paper claiming to understand the basic
principle of how DNNs work. He suggested that there are two phases that the
network goes trough while being trained - the fitting phase and the compression
phase. During the fitting phase the network memorizes the training data and
makes predictions based on that, during the compression phase the network
generalizes, it forgets the unnecessary information from the training data.
Tishby showed this by looking at DNNs trough the information domain, most
notably he used what is now called the information plane.
%TODO(ag939) possibly explain what an information plane is here.
Tishby made some interesting and significant claims about how DNNs work, however
he did no provide a formal proof, his conclusion are based only on experimental
evidence. 

In our work we looked at Tishby`s claim that DNNs compress data and throw away
information about the input. We reimplement his experiments as a form of
independent verification, showing that the results Tishby got are robust and not
a fluke of his specific parameters. We take a look at a paper by Saxe that
provides an opposing view to that of Tishby`s, explain why their experiments
disagree with Tishby as well as reproduce some of them. 

Lastly, We think Tishby`s experiments don't fully align with some of the ideas
he presented, specifically his idea that weights should be treated as if they
are random. We devised an experiment that tries to capture this idea more
explicitly that Tishby`s experiments.
