\documentclass[dissertation.tex]{subfiles}
\begin{document}

\paragraph{Abstract}
Neural Networks (NN) are an extremely successful tool, they are widely adopted
commercially and closely studied academically. However, even given the attention
they have there is no comprehensive understanding of how these models generalize
data and provide such impressive performance -- in fact very little is known
about how NN learn or about their inner workings. 

Recently Prof.\ Tishby produced a paper\cite{TISHBY} claiming to understand the
basic principles of how NN work. He decided to examine NNs through the
information domain. Tishby made the claim that the incredible performance of NNs
is due to their ability to compress information. Compressing data means the
network is only able to keep relevant input features and it must discard the
irrelevant bits of information, leading to ability to generalize. 

Tishby made interesting claims and provided experimental evidence to support his
claims. However, he did not provide a formal proof leaving his results up for
debate. A paper released by Saxe \cite{SAXE} has contested the claims made by
Tishby arguing that compression cannot happen in Neural Networks and Tishby's
results are a consequence of the Neural Network activation function Tishby used.
However, Saxe's paper suffers from the same problem as Tishby's as it does not
provide a formal proof only experimental evidence -- as such it does not settle
the rebuttal.

\paragraph{Our Contribution}
The task of the project was to reproduce the experiments presented in Tishby's
paper and show that they are robust -- not just an epiphenomenon of Tishby's
specific hyperparameters. During the project I have successfully implemented
tools that are able to reproduce the Tishby's and Saxe's experiments, as well as
leaving the tools open to extensions and future modifications. Using the tools I
was able to reproduce Tishby's experiment. I've managed to reproduce Tishby's
results exactly and show that they is robust to basic hyperparameters. However,
we were not able to reproduce Tishby's results if we varied the activation
function or the mutual information (MI) estimators. These results are similar to
that of Saxe's. However, our experiments disagreed with some of Saxe's
conclusions.

Tishby's method for estimating MI are quite primitive compared to
papers\cite{WGAO}\cite{SGAO}. Hence, the project was extended to be an
exploration of compression in Neural Networks. I investigated the role of Noise
in Neural Networks and Studied different method of MI estimation. My supervisor
and I devised a method "Batching" to potentially improve performance of existing
MI estimation techniques.

The project was originally meant to reproduce the experiments presented in
Tishby's paper and show that they are robust -- not just an epiphenomenon of
Tishby's specific hyperparameters.  In this project I have successfully
implemented tools that are able to reproduce Tishby's and Saxe's experiments, as
well as leaving them open to extensions for future experiments.  Thus I was able
reproduced Tishby's experiments -- and manged to achieve the same results, and
showed that they are robust to changes in hyperparameters. 

Overall, the project was a success. We have achieved aims set at the start of
the project. However, the results we got are not conclusive as we have not
provide a formal proof that support the correctness of our results. More work
needs to be done on the topic of compression in Neural Networks. Specifically,
we need new better tools for estimating mutual information -- the tools
currently available are provide different results. They are not fit to
conclusively answer questions that would help us to understand Neural Networks. 

\end{document}
