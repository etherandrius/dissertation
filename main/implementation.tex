
\section{Mutual Information Estimation}

As stated before we are trying to validate the fact that compression is
happening within the hidden layers in a neural network as such having robust
tools to measure mutual information is of utmost importance. 

\subsection{Mutual Information Definition}

Mutual information is a measure of dependence between two variables $ X $ and $
Y $, it quantifies the number of bits obtained about one when observing the
other.

Suppose we have two random variables $ X $ and $ Y $ if we want to measure
mutual information between the two of them we usually refer to one of the
following equations:

\medskip

Using the entropy of the distributions to infer the value
\begin{equation}
  I(X, Y) = H(X) - H(X|Y)
\end{equation}
\begin{equation}
  I(X, Y) = H(X) + H(Y) - H(X,Y)
\end{equation}

Or calculating the mutual information explicitly from probabilities, for
discrete and continuous distributions respectively
\begin{equation}
      {I} (X;Y)=\sum _{y\in {\mathcal {Y}}}\sum _{x\in {\mathcal
      {X}}}{p(x,y)\log {\left({\frac {p(x,y)}{p(x)\,p(y)}}\right)}} 
\end{equation}

\begin{equation}
    {I} (X;Y)=\int _{\mathcal {Y}}\int _{\mathcal {X}}{p(x,y)\log {\left({\frac
    {p(x,y)}{p(x)\,p(y)}}\right)}}\;dx\,dy
\end{equation}


\subsection{Theoretically undefined}

Calculating mutual information is mathematically defined for both continuous and
discrete distributions by using one of the functions above. However since we are
looking at neural networks we do not have the full distribution - only an
empirical sample of the unknown joint distribution $ P(X,Y) $. As such we cannot
use conventional methods to calculate the mutual information and instead have to
use tools to estimate it.

The field of estimating mutual information is relatively new as a result the
tools that are available are quite primitive. There has been some papers
published that use more advanced techniques to tackle the problem, we will talk
about in \autoref{ssection:advanced}

\section{Calculating Mutual Information}

\subsection{Experimental setup}

\begin{figure}[h]
    \begin{pythonfigure}
      Input:
      ;$T$; := number of layers
      ;$X$; := training data
      ;$Y$; := label data
      N := number of epochs
      Output: 
      ;$I_x$;(epoch, layer), # mutual information with $X$ 
      ;$I_y$;(epoch, layer)  # mutual information with $Y$ 
      Algorithm:
      NN = setup_neural_network(;$T$;, ;$X$;, ;$Y$;)
      for e in 0..N:
        NN.run_SGD_once()
        for t in 0..;$T$;:
          data = []
          for ;$x \in X$;:
            ;$\hat{t}$; = NN.predict(;$x$;).layer_activation(t)
            data.append(;$\hat{t}$;)
          ;$I_x$;(e, t) = calculate_mutual_information(data, ;$X$;)
          ;$I_y$;(e, t) = calculate_mutual_information(data, ;$Y$;)
    \end{pythonfigure}
    \caption{The general algorithm for calculating mutual information inside a
    neural network.}
\end{figure}

\begin{itemize}
  \item{
      X input is an empirical sample of all possible inputs.
    }
  \item{
      Y label data. Every $x \in X$ has a corresponding label $y \in Y$. 
    }
  \item{
      $T_{e,i}$ data for a specific epoch $e$ and specific layer $i$ in the
      neural network. The dataset is  generated by feeding every $x \in X$
      trough the neural network and recording the activations.
    }
\end{itemize}

\newpage

\subsection{Discrete method} 

  The method used by Tishby in his paper. It assumes that the discrete empirical
  distributions we manage to collect are a good representation of entropy with
  in the dataset. The method relies heavily on computing entropy values for
  dataset and it's subsets, as outlined in \autoref{eq:discrete}

\begin{equation}
  I(X, Y) = H(X) - H(X|Y) = H(X) - \sum _{y\in Y} H(X|Y = y)p(Y = y)
\end{equation} \label{eq:discrete}


  However just calculating mutual information for a discrete distribution is not
  enough as this yields mutual information equal to 0, in order to sidestep this
  issue we need to simulate randomness Tishby achieves this by grouping multiple
  values together. A more detailed explanation why this is done is given in
  \autoref{ssection:detnet}

   \autoref{fig:mutualinfo} trough \autoref{fig:entropy2} outline the full
   algorithm.
\begin{figure}[H]
    \begin{pythonfigure}
      Input: 
      ;$X = x_1, x_2,...,x_n$;
      ;$Y = y_1, y_2,...,y_n$;
      Output: ;$I(X:Y)$;
      ;$X$; = Batch close values of ;$X$; together
      ;$Y$; = Batch close values of ;$Y$; together
      ;$H(X)$; = Calculate entropy of X
      ;$H(X|Y)$; = Calculate conditional entropy of X given Y
      ;$I = H(X) - H(X|Y)$;
    \end{pythonfigure}
    \caption{Pseudo code for computing mutual information refer to
    \autoref{fig:entropy1} and \autoref{fig:entropy2} for entropy computation.}
    \label{fig:mutualinfo}
\end{figure}

\begin{figure}[H]
    \begin{pythonfigure}
      Input: ;$X = x_1, x_2,...,x_n$;
      Output: ;$H(X)$;
      for ;$ \hat{x}\in Unique(X)$;:
        count = 0       
        for ;$x \in X$;:
          if ;$\hat{x} = x$;:
            count = count + 1
        ;$P_x$; = count / len;$(X)$;
      ;$H(X) = - \sum _{x\in Unique(X)} P_x log(P_x)$; 
    \end{pythonfigure}
    \caption{Algorithm for computing entropy}
    \label{fig:entropy1}
\end{figure} 

\begin{figure}[H]
    \begin{pythonfigure}
      Input: 
      ;$X = x_1, x_2,...,x_n$;
      ;$Y = y_1, y_2,...,y_n$;
      Output: ;$H(X|Y)$;
      ;$H(X|Y) = H(X)$;
      for ;$ \hat{y}\in Unique(Y)$;:
        ;$\hat{X}$; = []
        for ;$x, y \in zip(X, Y)$;:
          if ;$\hat{y} = y$;:
            ;$\hat{X}$;.append(;$x$;)
          ;$H(X|Y) = H(X|Y) - H(\hat{X})$;
    \end{pythonfigure}
    \caption{Algorithm for computing conditional entropy}
    \label{fig:entropy2}
\end{figure}


\subsection{Kernel Density Estimation}

\subsection{Advanced methods} \label{ssection:advanced}
\subsubsection{Mutual Information by Gaussian approximation}
  failure
\subsubsection{Geometrical Adaptive Entropy Estimation}
  complete and utter failure

\section{Implementation Optimizations}


\begin{itemize}
  \item{
      parallelizing mutual information calculation. (good for kernel density
      estimation bad for discrete method.)
    }
  \item{
      calculating mutual information for multiple epochs at the same time. (bad
      for memory, good example for MNIST SET wasn't able to apply this method
      for it)
     parallelization
    }
  \item{
     choosing next epoch logic
    }
\end{itemize}



------------------------------------------------------------------------------------------------------------


Calculating mutual information is fully defined for the discrete and continuous
distributions, however we have a continuous distributions and a discrete sample
sets of those distributions. Our distributions are:
\begin{itemize}
  \item{
      $X$ - the input set (training set) to the NN (Neural Network)
    }
  \item{
      $T_i$ - activation of every NN layer, and
    }
  \item{
      $Y$ - the output set (labels of the training data)
    }
\end{itemize}


\begin{itemize}
  \item{
      As stated before Mutual information measurement is a contentious part of
      the project. Understanding why it's hard is useful in order to correctly
      evaluate the experiments.
    }
  \item{
      <++>
    }
\end{itemize}

\begin{itemize}
  \item{
      why Mutual Information Estimation is hard

      Yeah why?????

      Explain what it is - two continuous distributions we have a discrete
      sample 

      Why is it weird in this case
      our X is a discrete (sometimes continuous distribution) uniform
      distribution 

      A layer produces a continuous distribution

      Estimating mutual information boils down to how well we can measure
      entropy.

      Every layer is a vector, this does not affect theories for mutual
      information but makes the code more difficult to handle unless heavy
      abstraction is used.

      There has only been little work in this field so no publicly available
      collection of MIE (Mutual Information Estimators) exist, I was able to
      find some implementations of MIE by other researchers but they were not
      general enough or just didn't work. 

      There is some theoretical work in the subject but finding an
      implementation of the algorithms described in the paper has been difficult

      I was left with two options using the same methods as used by Tishby and
      Saxe or trying to implement an advanced algorithm from one of the papers.
      I tried to implement 

      In our specific example:
      \begin{itemize}
        \item{
            We need to produce mutual information values for:
            \begin{itemize}
              \item{
                  $I(X, T_i)$ - input and every NN layer
                }
              \item{
                  $I(Y, T_i)$ - output and every NN layer
                }
            \end{itemize}

            Where:

            \begin{itemize}
              \item{
                  $X$ - Input distribution
                }
              \item{
                  $Y$ - Output distribution
                }
              \item{
                  $T_i$ - Distribution of $i'th$ layer in the neural network
                }
            \end{itemize}

          }
        \item{
            Problem: we know that every input value is unique and that weight
            matrix for every layer in the NN is reversible, which implies that
            every input corresponds to a unique network activation, however that
            means that no information is lost in the network and compression
            does not happen. Which implies that for any single epoch mutual
            information between any two layers in NN or the input is just equal to the entropy of the input as H(X|Y) is always 

          }
        \item{
            <++>
          }
      \end{itemize}



    }
  \item{
      what have I tried
      \begin{itemize}
        \item{
            wgao - tried to implement, but failed the implementation was too
            complex and decided to cut my loses as it was taking too much time
            to implement. Emailed the author, he wasn't able to provide any
            code.
          }
        \item{
            wgao9 (lnn) - local nearest neighbour mutual information estimation.
            Based on a paper1 the code was available online on Github.
            The code produced nonsensical results highly sporadic and often even
            negative results for measure of mutual information. Although the
            code might have been fixable running it took an extremely long time
            so I've decided to not use this method as it already has consumed a
            lot of my time.
          }
      \end{itemize}
    }
  \item{
      what worked
      \begin{itemize}
        \item{
            Tishby - describe how Tishby calculated mutual information, what
            assumptions he made and what faults it has.
          }
        \item{
            Saxe used kernel density estimation in order to calculate entropy
            for some of their experiments, it a rigorous way to measure entropy
            however the results produced have big error bounds. see Kolchinsky
            and Tracey "Estimating Mixture Entropy with Pairwise Distances" for
            upper and lower bounds
          }
      \end{itemize}
    }
  \item{
      even though the MIE we ended up using is by far not the most sophisticated
      technique, it is still the state of the art that has been used in regards
      to measuring mutual information inside neural networks.
    }
\end{itemize}
    


\section{Tishby's reproduction}
\section{Saxe's reproduction}

