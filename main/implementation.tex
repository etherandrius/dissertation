
\section{Mutual Information Estimation}

As stated before we are trying to validate the fact that compression is
happening within the hidden layers in a neural network as such having robust
tools to measure mutual information is of utmost importance. 

\subsection{Mutual Information Definition}

Mutual information is a measure of dependence between two variables $ X $ and $
Y $, it quantifies the number of bits obtained about one when observing the
other.

Suppose we have two random variables $ X $ and $ Y $ if we want to measure
mutual information between the two of them we usually refer to one of the
following equations:

\medskip

Using the entropy of the distributions to infer the value
\begin{equation}
  I(X, Y) = H(X) - H(X|Y)
\end{equation}
\begin{equation}
  I(X, Y) = H(X) + H(Y) - H(X,Y)
\end{equation}

Or calculating the mutual information explicitly from probabilities, for
discrete and continuous distributions respectively
\begin{equation}
      {I} (X;Y)=\sum _{y\in {\mathcal {Y}}}\sum _{x\in {\mathcal
      {X}}}{p(x,y)\log {\left({\frac {p(x,y)}{p(x)\,p(y)}}\right)}} 
\end{equation}

\begin{equation}
    {I} (X;Y)=\int _{\mathcal {Y}}\int _{\mathcal {X}}{p(x,y)\log {\left({\frac
    {p(x,y)}{p(x)\,p(y)}}\right)}}\;dx\,dy
\end{equation}


\subsection{Theoretically undefined}

Calculating mutual information is mathematically defined for both continuous and
discrete distributions by using one of the functions above. However since we are
looking at neural networks we do not have the full distribution -- only an
empirical sample of the unknown joint distribution $ P(X,Y) $. As such we cannot
use conventional methods to calculate the mutual information and instead have to
use tools to estimate it.

The field of estimating mutual information is relatively new as a result the
tools that are available are quite primitive. There has been some papers
published that use more advanced techniques to tackle the problem, we will talk
about in \autoref{ssection:advanced}

\section{Calculating Mutual Information}

\subsection{Experimental setup}

\begin{figure}[H]
    \begin{pythonfigure}
      Input:
      ;$T$; = number of layers
      ;$X$; = training data
      ;$Y$; = label data
      N = number of epochs
      Output: 
      ;$I_x$;(epoch, layer), # mutual information with $X$ 
      ;$I_y$;(epoch, layer)  # mutual information with $Y$ 
      Algorithm:
      NN = setup_neural_network(;$T$;, ;$X$;, ;$Y$;)
      for e in 0..N:
        NN.run_SGD_once()
        for t in 0..;$T$;:
          data = []
          for ;$x \in X$;:
            ;$\hat{t}$; = NN.predict(;$x$;).layer_activation(t)
            data.append(;$\hat{t}$;)
          ;$I_x$;(e, t) = calculate_mutual_information(data, ;$X$;)
          ;$I_y$;(e, t) = calculate_mutual_information(data, ;$Y$;)
    \end{pythonfigure}
    \caption{The general algorithm for calculating mutual information inside a
    neural network.}
    \label{fig:general}
\end{figure}

\begin{itemize}
  \item{
      X input is an empirical sample of all possible inputs.
    }
  \item{
      Y label data. Every $x \in X$ has a corresponding label $y \in Y$. 
    }
  \item{
      $T_{e,i}$ data for a specific epoch $e$ and specific layer $i$ in the
      neural network. The dataset is  generated by feeding every $x \in X$
      trough the neural network and recording the activations.
    }
\end{itemize}

\newpage

\subsection{Discrete method} 

  The method used by Tishby in his paper. The method estimates mutual
  information between the $X$ or $Y$ and the hidden layer $T$ by assuming the
  observed empirical distribution of input samples is the true distribution. We
  use this assumption to compute entropies of $T$ and varying subsets as
  outlined in \autoref{eq:discrete}

\begin{equation}
  I(X, Y) = H(X) - H(X|Y) = H(X) - \sum _{y\in Y} H(X|Y = y)p(Y = y)
\end{equation} \label{eq:discrete}


  However just calculating mutual information for a discrete distribution is not
  enough as this yields mutual information equal to 0, in order to sidestep this
  issue we need to simulate randomness Tishby achieves this by grouping multiple
  values together, which he calls binning. A more detailed explanation why this
  is done is given in \autoref{ssection:detnet}

   \autoref{fig:mutualinfo} trough to \autoref{fig:entropy2} outline the full
   algorithm.
\begin{figure}[H]
    \begin{pythonfigure}
      Input: 
      ;$X = x_1, x_2,...,x_n$;
      ;$Y = y_1, y_2,...,y_n$;
      Output: ;$I(X:Y)$;
      ;$X$; = Bin close values of ;$X$; together
      ;$Y$; = Bin close values of ;$Y$; together
      ;$H(X)$; = Calculate entropy of X
      ;$H(X|Y)$; = Calculate conditional entropy of X given Y
      ;$I = H(X) - H(X|Y)$;
    \end{pythonfigure}
    \caption{Pseudo code for computing mutual information refer to
    \autoref{fig:entropy1} and \autoref{fig:entropy2} for entropy computation.}
    \label{fig:mutualinfo}
\end{figure}

\begin{figure}[H]
    \begin{pythonfigure}
      Input: ;$X = x_1, x_2,...,x_n$;
      Output: ;$H(X)$;
      for ;$ \hat{x}\in Unique(X)$;:
        count = 0       
        for ;$x \in X$;:
          if ;$\hat{x} = x$;:
            count = count + 1
        ;$P_x$; = count / len;$(X)$;
      ;$H(X) = - \sum _{x\in Unique(X)} P_x log(P_x)$; 
    \end{pythonfigure}
    \caption{Algorithm for computing entropy -- Discrete method}
    \label{fig:entropy1}
\end{figure} 

\begin{figure}[H]
    \begin{pythonfigure}
      Input: 
      ;$X = x_1, x_2,...,x_n$;
      ;$Y = y_1, y_2,...,y_n$;
      Output: ;$H(X|Y)$;
      ;$H(X|Y) = H(X)$;
      for ;$ \hat{y}\in Unique(Y)$;:
        ;$\hat{X}$; = []
        for ;$x, y \in zip(X, Y)$;:
          if ;$\hat{y} = y$;:
            ;$\hat{X}$;.append(;$x$;)
          ;$H(X|Y) = H(X|Y) - H(\hat{X})$;
    \end{pythonfigure}
    \caption{Algorithm for computing conditional entropy}
    \label{fig:entropy2}
\end{figure}

When computing mutual information between a hidden layer $T$ and the input set
$X$ we can abuse the fact that every element $x\in X$ is unique and uniquely
identifies an element $t\in T$ hence 

\begin{equation}
  H(T|X) = 0 
\end{equation}

\begin{equation}
  I(T, X) = H(T) - H(T|X) = H(T) 
\end{equation}

This does not affect the result but increases performance of our algorithm.

\subsection{Kernel Density Estimation}

  The KDE method used in Saxe's paper but originally devised by Kolchinsky \&
  Tracey (2017); Kolchinsky et al. (2017). As well as the Discrete method KDE
  assumes that the observed empirical distribution in the hidden layer $T$ is
  the true distribution. 

  However, instead of binning values together KDE assumes the distribution is a
  mixture of Gaussians. Using this fact we can get an upper bound when
  calculating entropy for a collection of data points.

  The algorithm closely follows figures :\autoref{fig:mutualinfo},
  \autoref{fig:entropy1}, and \autoref{fig:entropy2}, however there is a change
  the way entropy is calculated so instead of \autoref{fig:entropy1} we have
  \autoref{fig:entropy3} below.

\begin{figure}[H]
    \begin{pythonfigure}
      Input: 
      ;$X = x_1, x_2,...,x_n$;  
      var = noise variance 0.05 by default
      Output: ;$H(X)$;
      dists = compute distance matrix of ;$X$;
      dists = dists / 2*var # divide every distance by a value
      # an $x$ is an observation of a hidden layer so, hence it's a vector 
      # which has an associated dimension
      dim = ;$x_1$;.dimension
      normconst = (dim / 2)*log(2*;$\pi$;*var)
      lprobs = []
      for row in dists:
        lprobs.append(log(sum(exp(-row))) - log(n) - normconst
      H(X) = mean(lprobs) + (dim / 2)
    \end{pythonfigure}
    \caption{Algorithm for computing entropy -- Kernel Density Estimation method.
    The same algorithm as used by Saxe.}
    \label{fig:entropy3}
\end{figure} 

As before when computing mutual information between a hidden layer $T$ and the
input set $X$ we can use the fact that every element $x\in X$ is unique and
hence uniquely identifies an element in $t\in T$.

\subsection{Advanced methods} \label{ssection:advanced}

Since mutual information estimation is a contentious part of the project we
wanted to experiment with more advanced techniques however we were not able to
adopt the methods for this project, the methods that we have tried are outlined
below.

\subsubsection{Mutual Information by Gaussian approximation}
  
  "Estimating Mutual Information by Local Gaussian Approximation" (Shuyang Gao). A
  promising way to estimate mutual information. However the implementation
  proved to be to difficult and time consuming so we abandoned it. We reached
  out to the author but he was not able to provide any concrete code.


  failure
\subsubsection{Geometrical Adaptive Entropy Estimation}

  "Breaking the Bandwidth Barrier: Geometric Adaptive Entropy Estimation"
  (Weihao Gao). We were pointed to this paper by S. Gao the author of the
  previous method, as previously this looked like a promising method to measure
  entropy and mutual information. Furthermore, the code was available online
  unfortunately we were not able to adapt the code for multidimensional values
  as a result we were getting wrong and inconsistent results. We decided making
  this method work would require too much time and is out of scope of this
  project.

\section{Implementation Optimizations}

Producing data for the information plane requires a substantial amount of
computational power and memory, in order for the computation to complete in a
reasonable amount of time we have to utilize all the available resources and
minimize the amount of work we are doing.

\subsection{Maximising Resource Utilization}

The obvious way to maximise the resource usage is to parllelize the workload.
If we refer to \autoref{fig:general}, we can see two main ways we can do this.

The first way is to parallelize one of the two outer loops and run mutual
information calculations in parallel, this is easy to do, however an issue
occurs if we need a lot of memory since every instance of mutual information
calculation manipulates the datasets creating copies, this is an issue for
bigger datasets such as MNIST.

The second way is to parallelize the mutual information calculation itself, this
is nice since we use minimal amounts of memory, however might be hard or
impossible to implement as it depends on the method.

The second way is to parallelize the mutual information calculation itself, this
method has a  bonus that uses minimal amounts of memory improving performance
for bigger datasets such as MNIST. However implementing this option might be
hard as it heavily depends on the maths of the method, or impossible if the
method has an inherently linear part to it. In our case KDE parallel performance
is very good, however Tishby's Discrete method has some bottlenecks that I
wasn't able to remove.

\subsection{Minimising the Workload}

Even when using all the systems resources the calculations take a very long
time to complete as such we need to find a way to speed it up. If we consider an
information plane graph for ex. \autoref{fig:ip1}, we see that from epoch to
epoch there is very little change that is occurring, skipping some epochs might
be a good way to reduce workload while keeping the overall result unchanged. We
have implemented a couple of ways to skip the epochs 

\subsubsection{Simple Skip}
  
  A very simple simple way to skip epochs calculates mutual information for
  every $n'th$ epoch.

  It's quite effective and is fast to implement and easy to parallelize, however
  it yields subpar results. At the beginning of the training period during the
  fitting phase the step sizes are too big and yields gaps in data as there are
  big changes between consecutive epochs. Toward the end of the training period
  during the compression phase the step size becomes too small and we are
  wasting computation as the changes between epochs is negligible. The next two
  methods address this problem, but have their own drawbacks.

\subsubsection{Delta Skip -- Exact}

  The Exact Delta Skip method introduces a distance metric which measures
  mutual information distance between two epochs. Using the distance metric the
  method tries to skip as many epochs as possible while still guaranteeing that
  the distance is at most delta $\delta$, and backtracks when necessary.

  \subparagraph{Distance Metric}
  
  \autoref{fig:ip_pair}

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.80\textwidth]{figs/ip_pair.png}
    \captionof{figure}{
      Example how distance between two epochs is measured.
    }
    \label{fig:ip_pair}
  \end{figure}

\subsubsection{Delta Skip -- Approximate}

  The Exact method suffers the same problem as when we try to instantiate too
  many mutual information calculation instances, namely it runs out of memory if
  our dataset is too big. In order to solve this we use the approximate method
  which follows closely the Exact method. 

  The Algorithm -- refer to \autoref{fig:deltaapprox}, as previously, in the
  Exact method it uses a distance metric and measures every $n'th$ epoch where
  $n$ is adaptive and depends on how close the epochs are. The critical
  difference between Exact and Approximate method happens when the distance
  between epochs is more than $\delta$, the Exact method attempts to backtrack
  and fill in the gap whereas the Approximate doesn't backtrack and just
  continues to the next epoch, this is justified because the approximate method
  assumes that distance between epochs only shrinks and never increases.

\begin{figure}[H]
    \begin{pythonfigure}
      Input:
      prev = mutual information result of the previous epoch
      curr = mutual information result of the current epoch
      ;$\delta$; = user specified maximum "distance" between epochs
      Output:
      skip # how many epoch to skip until we measure again. defaults to 1
      Algorithm:
      dist = distance(prev, curr)
      if ;$\delta$; < dist: # the gap is bigger than $\delta$
        skip = skip
      else: # the gap is smaller than $\delta$
        skip = skip*2
    \end{pythonfigure}
    \caption{Delta Skip Approximate}
    \label{fig:deltaapprox}
\end{figure}

-----------------------------------------------------------------------------

\section{As If Random Experiment}

We beleive that Tishby's experiments Saxe's experiments could be improved, with
introducing a better notion of randomness. blah blah blah blah not finished.
