
\section{Mutual Information Estimation}

As stated before we are trying to validate the fact that compression is
happening within the hidden layers in a neural network as such having robust
tools to measure mutual information is of utmost importance. 

\subsection{Mutual Information Definition}

Mutual information is a measure of dependence between two variables $ X $ and $
Y $, it quantifies the number of bits obtained about one when observing the
other.

Suppose we have two random variables $ X $ and $ Y $ if we want to measure
mutual information between the two of them we usually refer to one of the
following equations:

\medskip

Using the entropy of the distributions to infer the value
\begin{equation}
  I(X, Y) = H(X) - H(X|Y)
\end{equation}
\begin{equation}
  I(X, Y) = H(X) + H(Y) - H(X,Y)
\end{equation}

Or calculating the mutual information explicitly from probabilities, for
discrete and continuous distributions respectively
\begin{equation}
      {I} (X;Y)=\sum _{y\in {\mathcal {Y}}}\sum _{x\in {\mathcal
      {X}}}{p(x,y)\log {\left({\frac {p(x,y)}{p(x)\,p(y)}}\right)}} 
\end{equation}

\begin{equation}
    {I} (X;Y)=\int _{\mathcal {Y}}\int _{\mathcal {X}}{p(x,y)\log {\left({\frac
    {p(x,y)}{p(x)\,p(y)}}\right)}}\;dx\,dy
\end{equation}


\subsection{Theoretically undefined}

Calculating mutual information is mathematically defined for both continuous and
discrete distributions by using one of the functions above. However since we are
looking at neural networks we do not have the full distribution - only an
empirical sample of the unknown joint distribution $ P(X,Y) $. As such we cannot
use conventional methods to calculate the mutual information and instead have to
use tools to estimate it.

The field of estimating mutual information is relatively new as a result the
tools that are available are quite primitive. There has been some papers
published that use more advanced techniques to tackle the problem. 

\subsection{Discrete method}

  Method used by Tishby in his paper 

\subsection{Kernel Density Estimation}

\subsection{Advanced methods}
\subsubsection{Mutual Information by Gaussian approximation}
  failure
\subsubsection{Geometrical Adaptive Entropy Estimation}
  complete and utter failure



------------------------------------------------------------------------------------------------------------


Mutual Information Estimation is a contentious part of the project. It is
valuable to understand how it's done before going in to details of how the
experiments are conducted.
 
We usually measure mutual information in one these three ways:

Using the entropy of the distributions
\begin{equation}
  I(X, Y) = H(X) - H(X|Y)
\end{equation}
\begin{equation}
  I(X, Y) = H(X) + H(Y) - H(X,Y)
\end{equation}
or calculating the mutual information explicitly from probabilities, for
discrete and continuous distributions respectively:
\begin{equation}
      {I} (X;Y)=\sum _{y\in {\mathcal {Y}}}\sum _{x\in {\mathcal
      {X}}}{p(x,y)\log {\left({\frac {p(x,y)}{p(x)\,p(y)}}\right)}} 
\end{equation}

\begin{equation}
    {I} (X;Y)=\int _{\mathcal {Y}}\int _{\mathcal {X}}{p(x,y)\log {\left({\frac
    {p(x,y)}{p(x)\,p(y)}}\right)}}\;dx\,dy
\end{equation}

Calculating mutual information is mathematically defined for continuous and
discrete distributions, however in our case we have an empirical sample of the
(unknown) continuous joint distribution $P(X, Y)$

Calculating mutual information is mathematically defined for continuous and
discrete distributions, however in our case we a discrete sample set of a
continuous distribution. Such a set up necessarily means that we will have some
missing information and our methods will contain errors in the results.

------------------------------------------------------------------------------------------------------------

Calculating mutual information is fully defined for the discrete and continuous
distributions, however we have a continuous distributions and a discrete sample
sets of those distributions. Our distributions are:
\begin{itemize}
  \item{
      $X$ - the input set (training set) to the NN (Neural Network)
    }
  \item{
      $T_i$ - activation of every NN layer, and
    }
  \item{
      $Y$ - the output set (labels of the training data)
    }
\end{itemize}


\begin{itemize}
  \item{
      As stated before Mutual information measurement is a contentious part of
      the project. Understanding why it's hard is useful in order to correctly
      evaluate the experiments.
    }
  \item{
      <++>
    }
\end{itemize}

\begin{itemize}
  \item{
      why Mutual Information Estimation is hard

      Yeah why?????

      Explain what it is - two continuous distributions we have a discrete
      sample 

      Why is it weird in this case
      our X is a discrete (sometimes continuous distribution) uniform
      distribution 

      A layer produces a continuous distribution

      Estimating mutual information boils down to how well we can measure
      entropy.

      Every layer is a vector, this does not affect theories for mutual
      information but makes the code more difficult to handle unless heavy
      abstraction is used.

      There has only been little work in this field so no publicly available
      collection of MIE (Mutual Information Estimators) exist, I was able to
      find some implementations of MIE by other researchers but they were not
      general enough or just didn't work. 

      There is some theoretical work in the subject but finding an
      implementation of the algorithms described in the paper has been difficult

      I was left with two options using the same methods as used by Tishby and
      Saxe or trying to implement an advanced algorithm from one of the papers.
      I tried to implement 

      In our specific example:
      \begin{itemize}
        \item{
            We need to produce mutual information values for:
            \begin{itemize}
              \item{
                  $I(X, T_i)$ - input and every NN layer
                }
              \item{
                  $I(Y, T_i)$ - output and every NN layer
                }
            \end{itemize}

            Where:

            \begin{itemize}
              \item{
                  $X$ - Input distribution
                }
              \item{
                  $Y$ - Output distribution
                }
              \item{
                  $T_i$ - Distribution of $i'th$ layer in the neural network
                }
            \end{itemize}

          }
        \item{
            Problem: we know that every input value is unique and that weight
            matrix for every layer in the NN is reversible, which implies that
            every input corresponds to a unique network activation, however that
            means that no information is lost in the network and compression
            does not happen. Which implies that for any single epoch mutual
            information between any two layers in NN or the input is just equal to the entropy of the input as H(X|Y) is always 

          }
        \item{
            <++>
          }
      \end{itemize}



    }
  \item{
      what have I tried
      \begin{itemize}
        \item{
            wgao - tried to implement, but failed the implementation was too
            complex and decided to cut my loses as it was taking too much time
            to implement. Emailed the author, he wasn't able to provide any
            code.
          }
        \item{
            wgao9 (lnn) - local nearest neighbour mutual information estimation.
            Based on a paper1 the code was available online on Github.
            The code produced nonsensical results highly sporadic and often even
            negative results for measure of mutual information. Although the
            code might have been fixable running it took an extremely long time
            so I've decided to not use this method as it already has consumed a
            lot of my time.
          }
      \end{itemize}
    }
  \item{
      what worked
      \begin{itemize}
        \item{
            Tishby - describe how Tishby calculated mutual information, what
            assumptions he made and what faults it has.
          }
        \item{
            Saxe used kernel density estimation in order to calculate entropy
            for some of their experiments, it a rigorous way to measure entropy
            however the results produced have big error bounds. see Kolchinsky
            and Tracey "Estimating Mixture Entropy with Pairwise Distances" for
            upper and lower bounds
          }
      \end{itemize}
    }
  \item{
      even though the MIE we ended up using is by far not the most sophisticated
      technique, it is still the state of the art that has been used in regards
      to measuring mutual information inside neural networks.
    }
\end{itemize}
    


\section{Tishby's reproduction}
\section{Saxe's reproduction}

