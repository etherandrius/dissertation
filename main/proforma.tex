\documentclass[dissertation.tex]{subfiles}
\begin{document}

{\large
\begin{tabular}{ll}
Name:               & \bf Andrius Grabauskas                    \\
College:            & \bf Robinson College                      \\
Project Title:      & \bf Measuring mutual information in Neural Networks \\
Examination:        & \bf Computer Science Tripos -- Part II, July 2001  \\
Word Count:         & \bf 9114\footnotemark[1] \\
Line Count:         & \bf 1303\footnotemark[2] \\
Project Originator: & Dr.\ Damon Wischik                  \\
Supervisor:         & Prof.\ Alan Mycroft                 \\ 
\end{tabular}
}
\footnotetext[1]{This word count was computed
by \texttt{detex *.tex | tr -cd '0-9A-Za-z $\tt\backslash$n' | wc -w}
}
\footnotetext[2]{This line count was computed 
by \texttt{wc -l **/*.py}
}
\stepcounter{footnote}


\section*{Original Aims of the Project}

The aim of the project was to reproduce Tishby's results, and see if they are
robust.As well as, explore ideas that Tishby presented in his paper, such as: 
\begin{itemize}
  \item{
      Compression in Neural Networks -- if a Neural Network can lose
      information, and if so how it happens, and under what assumptions.
    }
  \item{
      Compression Phase in Neural Networks -- Tishby claims the reason that
      Neural Networks generalize is because they learn how to compress the input
      representation. It would be interesting to explore if his claims hold up
      to scrutiny
    }
\end{itemize}

\section*{Work Completed}

Reproduced Tishby's code and experiments. Extended the code to reproduce
experiments conducted by Saxe. Explored advanced Mutual Information Estimators,
which proved to difficult to complete. Extended the code to explore a novel
mutual information estimation idea -- Batching. Produced code to plot the
Information Plane as well as visualize the process in a video format.

\section*{Special Difficulties}

None.

\newpage

\end{document}
