\documentclass[dissertation.tex]{subfiles}
\begin{document}

{\large
\begin{tabular}{ll}
Name:               & \bf Andrius Grabauskas                    \\
College:            & \bf Robinson College                      \\
Project Title:      & \bf Measuring mutual information in Neural Networks \\
Examination:        & \bf Computer Science Tripos -- Part II, July 2001  \\
Word Count:         & \bf 8398\footnotemark[1] \\
Line Count:         & \bf 1303\footnotemark[2] \\
Project Originator: & Dr.\ Damon Wischik                  \\
Supervisor:         & Prof.\ Alan Mycroft                 \\ 
\end{tabular}
}
\footnotetext[1]{This word count was computed
by \texttt{detex *.tex | tr -cd '0-9A-Za-z $\tt\backslash$n' | wc -w}
}
\footnotetext[2]{This line count was computed 
by \texttt{wc -l **/*.py}
}
\stepcounter{footnote}


\section*{Original Aims of the Project}

The aim of the project was to reproduce Tishby's results, and see if they
robust. Explore ideas that Tishby presented in his paper, such as: 
\begin{itemize}
  \item{
      Compression in Neural Networks -- if a Neural Network can lose
      information, if so how it happens what assumptions are made.
    }
  \item{
      Compression Phase in Neural Networks -- Tishby claims that the reason why
      Neural Networks generalize is because they learn how to compress the input
      representation. It would be interesting to explore if his claims hold to
      scrutiny
    }
\end{itemize}

\section*{Work Completed}

Reproduced Tishby's code and experiments. Extended the code to reproduce
experiments conducted by Saxe. Explored advanced Mutual Information Estimators,
which proved to difficult to complete. Extended the code to explore A Novel
Mutual Information Estimation Idea -- Batching. Produced code to plot the
Information Plane as well as visualize the process in a video format.

\section*{Special Difficulties}

None.

\newpage

\end{document}
