\documentclass[dissertation.tex]{subfiles}
\begin{document}

\begin{itemize}
  \item{
      Success Criteria
    }
  \item{
      we have successfully reproduced the results showed by Tishby and Saxe.
      However there are reasons to not trust either of them as they have flaws
      with them.
      \begin{itemize}
        \item{
            Tishby -- used only a toy dataset 
          }
        \item{
            Saxe -- Changed allot of parameters at once made the claim that no
            compression phase happens
          }
      \end{itemize}
    }
  \item{
      We need better tools for MIE
      \begin{itemize}
        \item{
            cannot judge subtleties if something has a compression phase our MIE
            are not trustesd
          }
        \item{
            we have seen KDE and Discrete show inconsistent results, when n'th
            layers hass less information about the input than the n+1'th layer.
          }
      \end{itemize}
    }
  \item{
      compression phase 
      \begin{itemize}
        \item{
            judging if the networks have a compression phase is moot point as of
            yet as tools for measuring information are not good enough. Case and
            point Saxe argues that there cannot be compression in NN but every
            every experiments show that compression exists.
          }
        \item{
            Saxe states that there is no compression in NN, however their
            experiments disagree. 
          }
        \item{
            we don't have the tools to say if the compression phase is actually
            happening
          }
        \item{
            Tishby says compression phase happens but he is using a toy dataset,
            which was shown to not have a compression phase by Saxe.
          }
      \end{itemize}
    }
  \item{
      performance
    }
\end{itemize}

---------------------------------------

\subsection{Deterministic networks} \label{ssection:detnet}

There's a very real argument to be made against compression in neural networks.
Consider a generic neural network we can think of it as a function that is a
series of matrix transformation, where a matrix corresponds to weights of a
specific layer. However these matrices are all random (at least at the start of
training) and hence probability of them being invertible is 100\%. 

Knowing that every single matrix is invertible allows us to conclude that that
neural network as a whole is an invertible function, which means no information
is lost and compression is impossible.

% matrix is invertible if it's random, maybe the neural network tends to
% non-invertible matrices ? if this was the case we would see mutual information
% full at the start and it would decrease over time - however we see that mutual
% information increases for Y and decreases for X


\subsection{Why Randomness is hard to capture} \label{ssection:rename}


\end{document}
