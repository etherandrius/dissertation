\documentclass[dissertation.tex]{subfiles}
\begin{document}

\section{Success Criteria}

The project was a success. I have met all the success criteria specified in the
proposal. That is I have reproduced the experiments presented in Tishby's paper.
I have produced a software suite that can run configurable experiments and can
be extended in the future.

\section{Extensions}

I have implemented a number of extensions for the project:

\begin{itemize}
  \item{
      Extended the code to accommodate KDE MIE, used in the paper by Saxe.
    }
  \item{
      Extended the code to accommodate for different datasets and added the
      MNIST dataset as an option. Adding more datasets requires writing code,
      but is easily done.
    }
  \item{
      Extended the code to be able to use different MIE on the same instance of
      a NN, in order to be able to compare how they perform. Previously to
      compare MIEs we either needed to save information for the whole training
      period of a NN or to launch two different instances of a NN with the same
      parameters, which might yield different results.
    }
  \item{
      Extended the code to improve performance.
      \begin{itemize}
        \item{
            Made the code parallel -- information for multiple epochs can be
            computed at the same time.
          }
        \item{
            Introduced adaptive ways to skip MI computations, which speed up
            computation significantly.
          }
      \end{itemize}
    }
  \item{
      Implemented movie plotting tools, which allow us to see how information
      plane changes over the training period. This allows us to better analyse
      the data and detect any issues we may have.
    }
\end{itemize}


\section{Tishby's Experiment}

\section{MIE bad}

\section{Saxe's experiment}

\paragraph{Tishby's Experiment} 

jEPRODUCED do images.

Tishby\cite{TISBHY} talks about a subset of the hyper parameters mentioned
above. 

Tishby varies the following 
\begin{itemize}
  \item{
      Even the implementation of the mathematical ideas can be considered a
      hyper parameter -- as it is unlikely that implementations follow the
      mathematical model exactly. This is the reason why independent
      verification is important and partly the reason of this thesis.
    }
  \item{
      Notes on comparing MIEs need to run on the same NN as they vary, need to
      run in real time as saving the whole network
    }
  \item{
      <++>
    }
  \item{
      <++>
    }
  \item{
      How to choose hyper parameters for binning, kde and batch. For the
      batching MIE, I have not
      investigated how we could choose a potential value of $b$ such that the
      range parameters $\theta$ for epochs in the range$[e,...,(e+b)]$ can be
      considered to be from the same probability distribution. And I am not
      aware of any method to choose the number of intervals in the Binning MIE
      or the noise size in KDE MIE.
    }
\end{itemize}

\section{MIE}

Inconsistent results between MIE.

\section{Ending remarks}
 
\begin{itemize}
  \item{
      Tishby and Saxe makes big claims about phases that a neural network is in
      NN. Such as "Is the compression phase an inherent part of NN and SGD
      algorithm" with out first agreeing on if compression can actually happen
      inside NNs.
    }
  \item{
      Furthermore, they are using very simple MIE.
    }
  \item{
      However they are using very simple MIE. In my opinion we need better tools
      before we can judge 
    }
\end{itemize}


\begin{itemize}
  \item{
      Success Criteria
    }
  \item{
      we have successfully reproduced the results showed by Tishby and Saxe.
      However there are reasons to not trust either of them as they have flaws
      with them.
      \begin{itemize}
        \item{
            Tishby -- used only a toy dataset 
          }
        \item{
            Saxe -- Changed allot of parameters at once made the claim that no
            compression phase happens
          }
      \end{itemize}
    }
  \item{
      We need better tools for MIE
      \begin{itemize}
        \item{
            cannot judge subtleties if something has a compression phase our MIE
            are not trustesd
          }
        \item{
            we have seen KDE and Discrete show inconsistent results, when n'th
            layers hass less information about the input than the n+1'th layer.
          }
      \end{itemize}
    }
  \item{
      compression phase 
      \begin{itemize}
        \item{
            judging if the networks have a compression phase is moot point as of
            yet as tools for measuring information are not good enough. Case and
            point Saxe argues that there cannot be compression in NN but every
            every experiments show that compression exists.
          }
        \item{
            Saxe states that there is no compression in NN, however their
            experiments disagree. 
          }
        \item{
            we don't have the tools to say if the compression phase is actually
            happening
          }
        \item{
            Tishby says compression phase happens but he is using a toy dataset,
            which was shown to not have a compression phase by Saxe.
          }
      \end{itemize}
    }
  \item{
      performance
    }
\end{itemize}


\end{document}
