\documentclass[11pt]{article}

\usepackage{minted}
\usepackage{hyperref}
\usepackage{datetime}
\usepackage{datenumber}
\usepackage{advdate}
\usepackage[super]{nth}
\usepackage[margin=0.75in]{geometry}

\parindent 0pt \parskip 6pt

\newcommand\todo[1]{\textbf{TODO(ag939): #1}}
\newcommand\haskell[1]{\mintinline{haskell}{#1}}
\newcommand\monospace[1]{\mintinline{text}{#1}}

\begin{document}

\thispagestyle{empty}

\centerline{\large Calculating Mutual Information in Deep Neural Networks}
\centerline{\large Progress Report}
\vspace{0.2in}

\centerline{Andrius Grabauskas, ag939}
\centerline{Robinson College}
\centerline{\today}

\vspace{0.2in}

\ThisYear{2019}\ThisMonth{1}\ThisDay{21}

\begin{tabular}[t]{@{}l}
{\bf Project Supervisor:} Dr.\ Damon Wischik \\[3mm]
{\bf Director of Studies:} Prof. Alan Mycroft
\end{tabular}
\hfill
\begin{tabular}[t]{@{}l @{}l}
{\bf Overseers: } & Dr.\ Robert Mullins \\[3mm] 
& Prof.\ Pietro Lio'
\end{tabular}

\vspace{0.3in}

\large{\bf Success Criteria}

  Success criteria have been achieved, I am now able to reproduce the results in
  Tishby`s paper\footnote{https://arxiv.org/abs/1703.00810}. That is, I am able
  to reproduce the information plane results that show the drift and the
  compression phase of a Neural Network for the specific parameters that Tishby
  used, and to show that the results are stable with variations to the parameters
  such as: batch size, network shape, training size. 

\large{\bf Current Progress, Planned work}

  I am not yet able to confirm or deny Tishby`s results when varying the Dataset
  neuron activation function or the Mutual Information estimators (MIE).
  There has been some difficulty getting sensible results from the Mutual
  Information estimators that I was able to find, and I have reason to believe
  that Tishby`s MIE might be flawed. The current plan in this regard is to try
  and use the same estimator that was used in Saxe`s paper
  \footnote{https://openreview.net/forum?id=ry\_WPG-A-} which used a Kernel
  Density estimator (KDE) and binning to compute the Mutual Information (MI). 
  Saxe`s paper conducted an experiment where they changed Dataset, activation
  function and the MIE, my current aim is to only vary the MIE which would show
  if Tishby`s result is only due to how he calculates MI.

  Also future plans include exploring the idea of how we can consider weights
  between layers as random variables, and test if Tishby`s hypothesis works
  under the those conditions. Tishby himself strongly suggests that weights
  should be considered as random variables but as of yet I am not aware of any
  work that tests this hypothesis. We are still unsure how it is best to test
  conduct such an experiment.

\end{document}
