\documentclass[11pt]{article}

\usepackage{minted}
\usepackage{hyperref}
\usepackage{datetime}
\usepackage{datenumber}
\usepackage{advdate}
\usepackage[super]{nth}
\usepackage[margin=0.75in]{geometry}

\parindent 0pt \parskip 6pt

\newcommand\todo[1]{\textbf{TODO(kc506): #1}}
\newcommand\haskell[1]{\mintinline{haskell}{#1}}
\newcommand\monospace[1]{\mintinline{text}{#1}}

\begin{document}

\thispagestyle{empty}

\centerline{\large Calculating Mutual Information in Deep Neural Networks}
\centerline{\large Progress Report}
\vspace{0.2in}

\centerline{Andrius Grabauskas, ag939}
\centerline{Robinson College}
\centerline{\today}

\vspace{0.2in}

\ThisYear{2019}\ThisMonth{1}\ThisDay{21}

\begin{tabular}[t]{@{}l}
{\bf Project Supervisor:} Dr.\ Damon Wischik \\[3mm]
{\bf Director of Studies:} Prof. Alan Mycroft
\end{tabular}
\hfill
\begin{tabular}[t]{@{}l @{}l}
{\bf Overseers: } & Dr.\ Robert Mullins \\[3mm] 
& Prof.\ Pietro Lio'
\end{tabular}

\vspace{0.3in}

\large{\bf Success Criteria}

  Success criteria have been achieved, I am now able to reproduce the results in
  Tishby`s paper\footnote{https://arxiv.org/abs/1703.00810}. That is, I am able
  to reproduce the information plane results that show the drift and the
  compression phase of a Neural Network for the specific parameters that Tishby
  used, and to show that the results are stable with variations to the parameters
  such as: batch size, network shape, training size. 


\large{\bf Current Progress, Planned work}

  I am not yet able to confirm or deny Tishby`s results when varying the Dataset
  activation function or the Mutual Information estimators (MIE), as there has
  been some difficulty getting sensible result from the Mutual Information
  estimators I was able to find, the current plan in this regard is to try to
  use the same estimator that was used in Saxe`s paper
  \footnote{https://openreview.net/forum?id=ry\_WPG-A-} and see what results the
  I can get with only varying the MIE as Saxe`s paper changed 
  
\end{document}
